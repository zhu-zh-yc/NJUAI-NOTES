# 大作业第二阶段实验报告

### 231830106 朱逸宸

#### 目录
1. [实验所用环境及参考文献](#一实验所用环境及参考文献)
2. [实验各模块及分析](#二实验各模块及分析)
    1. [Board & Game](#1-boardpy--gamepy)
    2. [TreeNodes & MCTS](#2-treenodepy--mctspy)
    3. [AIplayer](#3-aiplayerpy)
    4. ~~PolicyNN   
    (残差神经网络部分因tensorflow-gpu在不同电脑上适应不同而暂不展示)~~
    5. [MetaZeta](#4-metazetapy)
3. [实验效果分析](#三实验效果分析)



## 一、实验所用环境及参考文献
1. 实验环境：
    1. python
    2. numpy
    3. tkinter 
    4. tensorflow 
    5. threading 
    6. pydot
2. 参考文献：
    1. 链接: [五子棋强化学习](https://github.com/YoujiaZhang/AlphaGo-Zero-Gobang)  
    因本次大作业强度较大，第二阶段中 MCTS.py 以及 TreeNode.py 的代码部分为学生自己手搓复现，已经充分了解其原理。而PolicyNN部分的残差神经网络以及强化学习部分，则基本参考了网络中的代码，以实现更好的五子棋ai效果。五子棋为实现自我博弈学习，棋盘只使用了8*8的大小。  
    ![棋盘](..\images\棋盘.png)


## 二、实验各模块及分析
### 1. Board.py & Game.py
#### 1. 描述棋盘： from Board.py
```
 def __init__(self, width=8, height=8, n_in_row=5):
        # 设置棋盘的长宽
        self.width = width
        self.height = height

        # 棋盘状态《states》存储在dict中
        # key: 棋盘上的落子行为 value: 玩家
        # {38: 1, 23: 2, 24: 1, 36: 2, ··· }
        # 玩家1 落子 38 位置
        # 玩家2 落子 23 位置 ···
        self.states = {}
        
        self.n_in_row = n_in_row # 设置多少棋子连在一起获胜 (默认五子棋)
        self.players = [1, 2]    # player1 和 player2


def initBoard(self, start_player=0):
        """
        初始化棋盘
        start_player: 先手玩家
        """
        # 合理性判断
        if self.width < self.n_in_row or self.height < self.n_in_row:
            raise Exception('板宽和板高不得小于{}'.format(self.n_in_row))
        
        # 当前玩家设置
        #   默认先手的是玩家1
        self.current_player = self.players[start_player]

        # 在列表中保留还没有落子的位置
        self.availables = list(range(self.width * self.height))
        self.states = {}

        # 刚刚落子的情况
        self.last_move = -1 

def current_state(self):
        """
        以当前 玩家player 角度返回当前棋盘
        """
        # 使用 4*W*H 存储棋盘的状态
        square_state = np.zeros((4, self.width, self.height))
        if self.states:
            moves, players = np.array(list(zip(*self.states.items())))
            # moves 数组
            # 记录着两个玩家交错的落子位置

            move_curr = moves[players == self.current_player] # 当前玩家落子的位置
            move_oppo = moves[players != self.current_player] # 对手玩家落子的位置

            square_state[0][move_curr // self.width, move_curr % self.height] = 1.0 # 当前玩家所有落子棋盘
            square_state[1][move_oppo // self.width, move_oppo % self.height] = 1.0 # 对手玩家所有落子棋盘

            # 标记最近一次落子位置
            square_state[2][self.last_move // self.width, self.last_move % self.height] = 1.0

        if len(self.states) % 2 == 0:
            square_state[3][:, :] = 1.0 

        return square_state[:, ::-1, :]
```
#### 2. 描述游戏状态 from Board.py
```
def do_move(self, move):
        """
        落子
        move：落子的位置 一个整数 0 ~ H*W-1
        """
        self.states[move] = self.current_player
        self.availables.remove(move)  # 减少一个可以落子的位置

        # 切换玩家角色
        self.current_player = (
            self.players[0] if self.current_player == self.players[1]
            else self.players[1]
        )
        self.last_move = move # 记录上一次落子的位置

    def has_a_winner(self):
        """
        判断是否比出输赢
        """
        width = self.width
        height = self.height
        states = self.states
        n = self.n_in_row

        moved = list(set(range(width * height)) - set(self.availables))
        if len(moved) < self.n_in_row *2-1:
            return False, -1

        for m in moved:
            h = m // width
            w = m % width
            player = states[m]

            if (w in range(width - n + 1) and
                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):
                return True, player

            if (h in range(height - n + 1) and
                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):
                return True, player

            if (w in range(width - n + 1) and h in range(height - n + 1) and
                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):
                return True, player

            if (w in range(n - 1, width) and h in range(height - n + 1) and
                    len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):
                return True, player

        return False, -1

    def gameIsOver(self):
        """
        判断游戏是否结束
        """
        win, winner = self.has_a_winner()
        if win:
            return True, winner
        elif not len(self.availables):
            return True, -1
        return False, -1

    def getCurrentPlayer(self):
        return self.current_player
```
#### 3. 绘制棋盘与棋子 from Game.py
```
def Show(self, board, KEY=False):
        """
        绘制棋盘并显示游戏信息
        """
        x = board.last_move // board.width
        y = board.last_move % board.height
        self.drawPieces(player=board.current_player, rc_pos=(x,y), Index=len(board.states))
        
        if KEY:
            if self.flag_is_train == False:
                playerName = 'you'
                if board.current_player != 1:
                    playerName = 'AI'
            else:
                playerName = 'AI-'+str(board.current_player)

            self.drawText(str(len(board.states))+' '+playerName+':'+str(x)+' '+str(y))


    def drawText(self, string):
        '''
        游戏结束信息
        '''
        self.scrollText.insert(END, string+'\n')
        self.scrollText.see(END)
        self.scrollText.update()


    def drawPieces(self, player, rc_pos, Index, RADIUS=15, draw_rect=True):
        '''
        绘制棋子
        '''
        x, y = self.convert_rc_to_xy(rc_pos)
        colorText  = 'black' if player == 1 else 'white'
        colorPiece = 'white' if player == 1 else 'black'
        self.Canvas.create_oval(x-RADIUS, y-RADIUS, x + RADIUS, y+RADIUS, fill=colorPiece, outline=colorPiece)
        if draw_rect == True:
            if self.rect == None:
                OFFSET = 20
                self.rect = self.Canvas.create_rectangle(x-OFFSET, y-OFFSET, x+OFFSET, y+OFFSET, outline="#c1005d")
                self.rect_xy_pos = (x, y)
            else:
                rc_pos = self.convert_xy_to_rc((x, y))
                old_x, old_y = self.rect_xy_pos
                new_x, new_y = self.convert_rc_to_xy(rc_pos)
                dx, dy = new_x-old_x, new_y-old_y
                self.Canvas.move(self.rect, dx, dy)
                self.rect_xy_pos = (new_x, new_y)
        self.Canvas.create_text(x,y, text=str(Index), fill=colorText,)
        self.Canvas.update()
```
#### 4.描述落子位置 from Game.py
```
def convert_rc_to_xy(self, rc_pos):
        # 传入棋子在棋盘上的r、c值，传出其在canvas上的坐标位置
        SIDE = (435 - 400)/2
        DELTA = (400-2)/(self.boardWidth-1)
        r, c = rc_pos
        x = c*DELTA+SIDE
        y = r*DELTA+SIDE
        return x, y
    def convert_xy_to_rc(self, xy_pos):
        # 传入xy值，传出rc坐标
        SIDE = (435 - 400)/2
        DELTA = (400-2)/(self.boardWidth-1)
        x, y = xy_pos
        r = round((y-SIDE)/DELTA)
        c = round((x-SIDE)/DELTA)
        return r, c
```

#### 5. 游戏模式（自我对弈&人机对弈） from Gmae.py
```
# 1、自我对弈
def selfPlay(self, player, Index=0):
        """ 
        构建一个AI自我博弈，重用搜索树，并存储自玩数据：(棋盘状态, 落子概率, 胜者预测) 以供训练
        """
        
        self.board.initBoard()
        boards, probs, currentPlayer = [], [], []

        while True:
            # 当前玩家，针对当前棋盘根据MCTS获取下一步的落子行为
            move, move_probs = player.getAction(self.board, self.flag_is_train)
            
            # 存储下棋的数据
            boards.append(self.board.current_state())
            probs.append(move_probs)
            currentPlayer.append(self.board.current_player)

            # 下棋落子
            self.board.do_move(move)

            # # 展示下棋的过程
            if self.flag_is_shown:
                self.Show(self.board)
            
            # 是否已经结束
            gameOver, winner = self.board.gameIsOver()

            if gameOver:
                # 根据最终游戏的结果，构造用于训练神经网络的《标签》Z
                winners_z = np.zeros(len(currentPlayer))
                if winner != -1:
                    winners_z[np.array(currentPlayer) == winner] = 1.0
                    winners_z[np.array(currentPlayer) != winner] = -1.0

                # 重新设置MCTS，初始化了整棵树
                player.resetMCTS()

                if self.flag_is_shown:
                    if winner != -1:
                        if self.flag_is_train == False:
                            playerName = 'you'
                            if self.board.current_player != 1:
                                playerName = 'AI'
                        else:
                            playerName = 'AI-'+str(self.board.current_player)
                        self.drawText("Game end. Winner is : "+str(playerName))
                    else:
                        self.drawText("Game end. Tie")

                self.rect = None
                return winner, zip(boards, probs, winners_z)

# 2、人机对弈
def humanMove(self, event):
        self.flag_human_click = True
        x, y = event.x, event.y
        r, c = self.convert_xy_to_rc((x, y))
        # 下一步棋
        self.move_human = r*self.boardWidth + c

    def playWithHuman(self, player):
        """ 
        与人对弈
        """
        self.Canvas.bind("<Button-1>", self.humanMove)
        # 先手玩家为 第0号玩家
        self.board.initBoard(0)

        KEY = 0
        while True:
            if self.board.current_player == 1:
                move, move_probs = player.getAction(self.board, self.flag_is_train)
                # 落子
                self.board.do_move(move)
                KEY = 1
            else:
                if self.flag_human_click:
                    if self.move_human in self.board.availables:
                        self.flag_human_click = False
                        self.board.do_move(self.move_human)
                        KEY = 1
                    else:
                        self.flag_human_click = False
                        print("无效区域")

            if self.flag_is_shown and KEY == 1:
                self.Show(self.board)
                KEY = 0

            gameOver, winner = self.board.gameIsOver()
            if gameOver:
                # 更新设置MCTS
                player.resetMCTS()
                if self.flag_is_shown:
                    if winner != -1:
                        if self.flag_is_train == False:
                            playerName = 'you'
                            if self.board.current_player != 1:
                                playerName = 'AI'
                        else:
                            playerName = 'AI-'+str(self.board.current_player)
                        self.drawText("Game end. Winner is : "+str(playerName))
                    else:
                        self.drawText("Game end. Tie")
                # self.resetBoard()
                break
        self.rect = None
```
---
### 2. TreeNode.py & MCTS.py
#### 1. 对MCTS的介绍
![MCTS介绍](..\images\MCTS介绍.png)
```
class TreeNode():
    """
    蒙特卡洛搜索树的节点类
    """
    def __init__(self, parent, prior_p):
        self.NUM = 1
        self.father = parent # 父节点
        self.children = {}   # 孩子节点
        self.N_visits = 0    # 该节点的访问次数
        self.Q = 0           # 节点的总收益(V) / 总访问次数(N)
        self.U = 0           # 神经网络学习的目标：U 是正比于概率P，反比于访问次数N
        self.P = prior_p     # 走某一步棋(a)的先验概率
```

#### 2、MCTS树的定义 from TreeNodes.py
```
# 2.1 选择
def getValue(self, factor):
        """
        计算每个节点的《价值》，用以选择
        """
        # factor 是一个从0到正无穷的调节因子
        #------------------------------------------------------------------------------
        # 如果 factor 越小，MCTS 搜索中的探索广度就越低，对神经网络输出的先验概       
        # 率的关注就越少，导致性能不理想。如果 factor 太大，探索广度就太高了，它太依
        # 赖于神经网络输出的先验概率。它不太重视 MCTS 模拟得到的结果，性能也不
        # 理想。因此，需要一个合理的 factor 值
        self.U = (factor * self.P *np.sqrt(self.father.N_visits) / (1 + self.N_visits))
        return self.Q + self.U

def select(self, factor):
        """
        选择：根据《策略》选择落子动作
        """
        # 选择所有孩子中《分数》最高
        return max(self.children.items(), key=lambda act_node: act_node[1].getValue(factor))


# 2.2 拓展
def expand(self, action_priors):
        """
        扩展：增加叶子节点的孩子
        """
        # action_priors：(落子动作a，该位置的概率p)
        for action, prob in action_priors:
            if action not in self.children:
                self.children[action] = TreeNode(self, prob)

# 2.3 模拟
 通过蒙特卡洛方法随机采样来预测 action，prob。

# 2.4 更新
def update(self, leaf_value):
        """
        更新：更新节点的数值
        """
        # leaf_value: 从当前选手的身份《评估》叶子节点的价值，也就是走这一步预计带来的收益。
        self.N_visits += 1
        self.Q += 1.0*(leaf_value - self.Q) / self.N_visits

    def updateRecursive(self, leaf_value):
        """
        回溯：递归更新从叶子到根上的所有节点
        """
        # 如果这个节点是有父亲，优先更新该节点的父亲
        if self.father:
            self.NUM = 0
            for i in list(self.children.items()):
                self.NUM += i[1].NUM
            self.father.updateRecursive(-leaf_value)
        self.update(leaf_value)
    
    def isLeaf(self):
        """
        判断该节点是不是叶子节点：没有孩子的就是叶子
        """
        return self.children == {}

    def isRoot(self):
        """
        判断该节点是不是根节点：没有父亲的就是根
        """
        return self.father is None
```
#### 3、MCTS树的实现 from MCTS.py
```
# 3.1推演
def playout(self, state):
        """
        推演： 从根到叶进行推演，在叶上获取一个值，并通过其父级将其传播回来。
        """
        node = self.root
        while True:
            # 如果，是叶子节点就跳出
            if node.isLeaf():
                break
            # 否则
            #   就选择该节点所有孩子中《分数》最高的。
            #       action：落子动作
            #       node：孩子节点
            action, node = node.select(self.fator)
            state.do_move(action)
        # ---------------------------------------------------
        # 在原本的MCTS中，《评估》操作采用蒙特卡洛的方法，通过随机下棋
        # 模拟走完一次完整棋局 (称为 rollout), 得到胜负结果。模拟的结果反应在 leaf_value 变量中
        # ---------------------------------------------------
        # 根据当前的《状态(棋盘)》使用神经网络预测：下一步所有的动作以及
        # 对应的概率 + 此步未来的收益
        action_probs, leaf_value = self.policy_NN(state)

        # 检查一下当前《状态》是不是已经分出胜负
        gameOver, winner = state.gameIsOver()

        # 如果这盘棋还没结束
        if not gameOver:
            # 扩展当前节点
            node.expand(action_probs)
        else:
            # 平局
            if winner == -1:
                leaf_value = 0.0
            else:
                # 如果《模拟/预测》的结果，获胜的是当前的玩家，+1分 否则 -1分
                leaf_value = (
                    1.0 if winner == state.getCurrentPlayer() else -1.0
                )
        
        # 根据神经网络的预测结果 leaf_value
        # 自下而上《更新》叶子
        node.updateRecursive(-leaf_value)

# 3.2 落子
def getMoveProbs(self, state, flag_is_train):
        # print(str(self.root))
        """
        获取落子依据：按顺序进行所有推演，并返回可用操作及其相应的概率。
        """
        # state：当前游戏棋盘的状态。
        # 在（0，1）中控制探测(exploration)程序
        exploration = 1.0 if flag_is_train else 1e-3

        # 根据当前棋盘状态，经过 simulations 次数的模拟
        # 构建出了一个 MCTS 树，根节点是依托于当前棋盘。
        for _ in range(self.simulations):
            state_copy = copy.deepcopy(state)
            self.playout(state_copy)

        # 根据 MCTS 根节点，获取下一步落子的决策
        act_visits = [(act, node.N_visits) for act, node in self.root.children.items()]
        acts, visits = zip(*act_visits)
        act_probs = softmax(1.0/exploration * np.log(np.array(visits) + 1e-10))

        # 落子的位置 + 每一个位置的胜率
        return acts, act_probs

# 3.3 更新
def updateMCTS(self, move):
        # 如果走的这一步在就是当前树根的孩子之一
        if move in self.root.children:
            # print('**********************************')
            # print('延续MCTS : ',str(self.root),'----->',str(self.root.children[move]))
            # 延续这棵树，更新树根
            self.root = self.root.children[move]
            self.root.father = None
        else:
            # 只有当整个对局分出胜负的时候，才重置整棵MCTS
            self.root = TreeNode(None, 1.0)
            # print('**********************************')
            # print('重新设置MCTS',str(self.root))
```

### 3. AIplayer.py
落子
```
 def getAction(self, board, flag_is_train):
        # 获得当前棋盘中可以落子的地方
        emptySpacesBoard = board.availables

        # move_probs 的尺寸是整个棋盘的大小
        # 每一个格子上存放着此处落子的概率
        move_probs = np.zeros(board.width * board.height)

        if len(emptySpacesBoard) > 0:
            # 基于 MCTS 获取下一步的落子行为，以及对应每一个位置胜率
            acts, probs = self.MCTS.getMoveProbs(board, flag_is_train)
            move_probs[list(acts)] = probs
            
            if flag_is_train:
                # 添加《Dirichlet Noise》进行探索（自我对弈训练所需）
                move = np.random.choice( # 随机抽取
                    acts, # 落子行为
                    p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs)))
                )
                # 自下而上更新根节点并重用 MCTS
                # AI 相当于是《同一个人左右互搏》使用同一棵MCTS进行对我对弈
                self.MCTS.updateMCTS(move)

            else:
                # 非训练
                # ------------------------------------
                # 更新根节点并使用默认的temp=1e-3重用搜索树
                # 这几乎等同于选择prob最高的移动
                move = np.random.choice(acts, p=probs)
                # 重置 MCTS
                self.MCTS.updateMCTS(-1)
            
            # 依据概率选择下一步落子的位置，以及当前棋盘的所有位置的概率（分布）
            return move, move_probs
        else:
            print("WARNING: the board is full")

    def __str__(self):
        return "MCTS {}".format(self.player)
```

### 4. MetaZeta.py
#### 1.GUI
```
def __init__(self, flag_is_shown = True, flag_is_train = True):
        self.flag_is_shown = flag_is_shown
        self.flag_is_train = flag_is_train
        
        self.window = tk.Tk()
        self.window.resizable(0,0)
        self.window.title('Meta Zeta --- Zhuzh')
        self.window.geometry('810x500')

        self.btStart = tk.Button(self.window , text='开始', command=lambda :self.thredaTrain(self.train))
        self.btStart.place(x=480, y=10)
        
        self.btReset = tk.Button(self.window , text='重置', command = self.resetCanvas)
        self.btReset.place(x=540, y=10)

        self.iv_default = IntVar()
        self.rb_default1 = Radiobutton(self.window, text='AI 自我对弈', value=1, variable=self.iv_default)
        self.rb_default2 = Radiobutton(self.window, text='与 AI 对战', value=2,  variable=self.iv_default)
        self.rb_default1.place(x=595, y=15)
        self.rb_default2.place(x=695, y=15)
        self.iv_default.set(2)

        self.canvas = tk.Canvas( self.window, bg='#CD853F', height=435, width=435)
        self.scrollText = scrolledtext.ScrolledText(self.window, width=38, height=24)

        # 构建棋盘
        self.game = Game(Canvas=self.canvas, scrollText=self.scrollText, flag_is_shown=self.flag_is_shown, flag_is_train=self.flag_is_train)
        # 构建神经网络
        self.NN = PolicyValueNet((4, self.game.boardWidth, self.game.boardHeight))

        # 构造MCTS玩家，将神经网络辅助MCTS进行决策
        self.MCTSPlayer = MCTSPlayer(policy_NN = self.NN.policy_NN)
        # 输出模型结构
        plot_model(self.NN.model, to_file='model.png', show_shapes=True)

        self.DrawCanvas((30, 30))
        self.DrawText((480, 50))
        self.DrawRowsCols((42, 470), (10, 35))

        self.window.mainloop()

def DrawCanvas(self, canvas_pos):
        x, y = canvas_pos
        # 画纵横线
        for i in range(self.game.boardWidth+1):
            pos = i*(400-2)/(self.game.boardWidth-1)
            SIDE = (435 - 400)/2
            self.canvas.create_line(SIDE, SIDE+pos, SIDE+400, SIDE+pos)
            self.canvas.create_line(SIDE+pos, SIDE, SIDE+pos, SIDE+400)
        self.canvas.place(x=x, y=y)

    def DrawRowsCols(self, rspos, cspos):
        rx, ry = rspos
        cx, cy = cspos
        for i in range(8):
            clabel = tk.Label(self.window, text=str(i))
            clabel.place(x=cx, y=cy+i*(400-2)/(self.game.boardWidth-1))

            rlabel = tk.Label(self.window, text=str(i))
            rlabel.place(x=rx+i*(400-2)/(self.game.boardWidth-1), y=ry)

    def DrawText(self, xy_pos):
        x, y = xy_pos
        self.scrollText.place(x=x, y=y)
    
    def drawScrollText(self, string):
        self.scrollText.insert(END, string+'\n')
        self.scrollText.see(END)
        self.scrollText.update()
    
    def resetCanvas(self):
        self.canvas.delete("all")
        self.scrollText.delete(1.0, END)
        self.DrawCanvas((30, 30))
        return
```

#### 2、多线程
```
def thredaTrain(self, func,):
        # 将函数打包进线程
        myThread = threading.Thread(target=func,) 
        myThread.setDaemon(True) 
        myThread.start()
```

#### 3、自我训练
```
def train(self):
        Loss = []
        if self.iv_default.get() == 1:
            self.flag_is_train = True
            self.game.flag_is_train = True
        else:
            self.flag_is_train = False
            self.game.flag_is_train = False

        # 总共进行 MAX_Games 场对弈
        for oneGame in range(self.MAX_Games):
            # start = time.time()
            if self.flag_is_train:
                # MCTS 进行自我对弈
                self.drawScrollText('正在 第'+str(oneGame+1)+'轮 自我对弈···')
                winner, play_data = self.game.selfPlay(self.MCTSPlayer,Index=oneGame+1)

                # 为神经网络存储 训练数据
                self.NN.memory(play_data)

                # 如果数据池已经足够了《一批数据》的量，就对决策网络进行参数更新（训练）
                if len(self.NN.trainDataPool) > self.NN.trainBatchSize:
                    loss = self.NN.update(scrollText=self.scrollText)
                    Loss.append(loss)
                else:
                    self.drawScrollText("收集训练数据: %d%%" % (len(self.NN.trainDataPool)/self.NN.trainBatchSize*100))

                # 每过一定迭代次数保存模型
                if (oneGame+1) % self.save_ParaFreq == 0:
                    self.NN.save_model('models/'+str(oneGame+1)+'policy.model')
                    self.drawScrollText("保存模型")
                
                self.canvas.delete("all")
                self.DrawCanvas((30, 30))
            else:
                if not self.flag_is_train:
                    self.NN.load_model(r"朱逸宸 231830106 大作业\models\2000policy.model") 
                    self.drawScrollText("读取模型")
                self.game.playWithHuman(self.MCTSPlayer,)
                return
            
            # 重置画布
            # end = time.time()
            # print("循环运行时间:%.2f秒"%(end-start))
```
---
## 三、实验效果展示
#### 1. 自我对弈效果展示  
![机器自我对弈](..\images\演示1.gif)
#### 2. 人机对弈效果展示 
![人机对弈](..\images\演示2.gif)







